{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlgYrROy0+boaKTfRDj78+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/3m6d/Computer_vision/blob/main/Week6_finetuning_using_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HXq22UnoJ5RX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import time"
      ],
      "metadata": {
        "id": "37liFUzFV3HN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cpu_count = os.cpu_count()"
      ],
      "metadata": {
        "id": "sFgd9hTwWKdF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6mh84AiWQWF",
        "outputId": "e2985bd3-ea44-4ff0-db2e-0b48933cd73e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = '/content/drive/MyDrive/Cat_dog_images.zip'"
      ],
      "metadata": {
        "id": "vgSn98KLWofd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/data')"
      ],
      "metadata": {
        "id": "aH6s7OSmWwXY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/data/Cat_dog_images'"
      ],
      "metadata": {
        "id": "xCx1cA7FWyca"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224"
      ],
      "metadata": {
        "id": "R0WV07tlW2r6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split #random splits data"
      ],
      "metadata": {
        "id": "1dOZW_iYW6vx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loaders(data_dir, input_size=IMG_SIZE, batch_size=32, val_split=0.2 ):#validation split is 20%\n",
        "\n",
        "\n",
        "# data augementation is where the data is changed by increasing the brightness\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((input_size, input_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "  # in the following code, the data images is mapped for categorical values to index\n",
        "\n",
        "    dataset = datasets.ImageFolder(data_dir, transform=transform) # dataset is torch.utils.data.Dataset object\n",
        "    print(\"Dataset classes names:\", dataset.classes)\n",
        "    print(\"Classes and it's mapped index:\", dataset.class_to_idx)\n",
        "    print(\"\")\n",
        "\n",
        "    dataset_size = len(dataset)\n",
        "    print(\"Total number of images in the dataset:\", dataset_size)\n",
        "    print(\"\")\n",
        "\n",
        "    val_size = int(val_split * dataset_size)\n",
        "    train_size = dataset_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size]) # both are still PyTorch dataset object\n",
        "    print(\"Train dataset size:\", len(train_dataset))\n",
        "    print(\"Validation dataset size:\", len(val_dataset))\n",
        "    print(\"\")\n",
        "\n",
        "    # The DataLoader is what actually feeds data into the model in batches during training.\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=cpu_count)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=cpu_count)\n",
        "\n",
        "    print(\"Datatype of train_loader:\", type(train_loader))\n",
        "    inputs, labels = next(iter(train_loader))\n",
        "    print(inputs.shape)\n",
        "    print(labels.shape)\n",
        "\n",
        "    dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "    dataset_sizes = {'train': train_size, 'val': val_size}\n",
        "    class_names = dataset.classes\n",
        "\n",
        "    return dataloaders, dataset_sizes, class_names"
      ],
      "metadata": {
        "id": "TGZEO3HxW_gR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dipawoli = get_data_loaders(data_dir, input_size=IMG_SIZE, batch_size=32, val_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dIxYxD_XLeA",
        "outputId": "05f508da-e26c-4558-96d6-1ec42095cf3e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset classes names: ['cats', 'dogs']\n",
            "Classes and it's mapped index: {'cats': 0, 'dogs': 1}\n",
            "\n",
            "Total number of images in the dataset: 210\n",
            "\n",
            "Train dataset size: 168\n",
            "Validation dataset size: 42\n",
            "\n",
            "Datatype of train_loader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders, dataset_sizes, class_names = get_data_loaders(data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxPa0kBXYhyB",
        "outputId": "55268fa0-6732-43a1-ff8a-07ea15492733"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset classes names: ['cats', 'dogs']\n",
            "Classes and it's mapped index: {'cats': 0, 'dogs': 1}\n",
            "\n",
            "Total number of images in the dataset: 210\n",
            "\n",
            "Train dataset size: 168\n",
            "Validation dataset size: 42\n",
            "\n",
            "Datatype of train_loader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evry batch has 32 images with 3 channel with 224 by 224 size"
      ],
      "metadata": {
        "id": "KKxoMaKeZezX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, dataset_sizes, device, classification_type, num_epochs=10, lr=0.001):\n",
        "    if classification_type == 1:\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "    else:\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        #Each epoch is one full pass over the dataset.\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "          # Two phases per epoch:\n",
        "          # train: Model learns and updates weights.\n",
        "          # val: Model evaluates accuracy on unseen data, no learning.\n",
        "\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device).float().unsqueeze(1) # this is for binary classification for unsqueeze\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                # reset the gradient to zero\n",
        "                # THIS MUST NOT BE AVOIDED!\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    # forward prop\n",
        "                    if classification_type == 1:\n",
        "                        outputs = torch.sigmoid(outputs)\n",
        "                        preds = (outputs > 0.5).float()\n",
        "                    else:\n",
        "                      _, preds = torch.max(outputs, 1)\n",
        "                    # getting the predicted class and computing loss.\n",
        "\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        # backpropagation and updating the parameters\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            # calculating each epoch loss and accuracy and normalizes by total number of sample.\n",
        "\n",
        "            print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "qwF1NguAZqgh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa8DkP8nZ6BK",
        "outputId": "6409b8fa-89a3-40bd-9783-92550d8b2ec7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'resnet18'  # Change as needed\n",
        "if len(class_names) == 2:\n",
        "    num_classes = 1\n",
        "else:\n",
        "    num_classes = len(class_names)\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "# model = get_model(model_name, num_classes=num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqaadPh4aBjh",
        "outputId": "faa467b9-2bb1-4a85-f6a4-b0efedb2bdbc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 72.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = train_model(model, dataloaders, dataset_sizes, device, num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZZNne31aE7t",
        "outputId": "31146cd5-5845-4ec6-e067-37ab91a52c9e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "--------------------\n",
            "Train Loss: 0.6084 Acc: 0.8393\n",
            "Val Loss: 0.6041 Acc: 0.7143\n",
            "Epoch 2/10\n",
            "--------------------\n",
            "Train Loss: 0.5743 Acc: 0.8631\n",
            "Val Loss: 0.6729 Acc: 0.7143\n",
            "Epoch 3/10\n",
            "--------------------\n",
            "Train Loss: 0.5610 Acc: 0.8929\n",
            "Val Loss: 0.5959 Acc: 0.7619\n",
            "Epoch 4/10\n",
            "--------------------\n",
            "Train Loss: 0.5517 Acc: 0.9226\n",
            "Val Loss: 0.7158 Acc: 0.6190\n",
            "Epoch 5/10\n",
            "--------------------\n",
            "Train Loss: 0.5392 Acc: 0.9345\n",
            "Val Loss: 0.7198 Acc: 0.6190\n",
            "Epoch 6/10\n",
            "--------------------\n",
            "Train Loss: 0.5341 Acc: 0.9524\n",
            "Val Loss: 0.5507 Acc: 0.8571\n",
            "Epoch 7/10\n",
            "--------------------\n",
            "Train Loss: 0.5358 Acc: 0.9464\n",
            "Val Loss: 0.5569 Acc: 0.8571\n",
            "Epoch 8/10\n",
            "--------------------\n",
            "Train Loss: 0.5206 Acc: 0.9881\n",
            "Val Loss: 0.5611 Acc: 0.8810\n",
            "Epoch 9/10\n",
            "--------------------\n",
            "Train Loss: 0.5197 Acc: 0.9881\n",
            "Val Loss: 0.5084 Acc: 0.9524\n",
            "Epoch 10/10\n",
            "--------------------\n",
            "Train Loss: 0.5168 Acc: 1.0000\n",
            "Val Loss: 0.5914 Acc: 0.6905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YhY8vHmIaHt7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}